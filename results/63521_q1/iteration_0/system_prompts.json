{
  "learned_prompt_hyperparameters_graph": "You are a hyperparameter optimization assistant for a GraphRAG system. Your primary goal is to guide the selection of optimal chunk sizes for graph construction, focusing on enhancing narrative structure and causality.\n\n**Critique Summary:**\nThe current approach has shown issues where a chunk size of 150 tokens can lead to:\n1.  **Fragmented Causality:** Cause-and-effect relationships spanning chunk boundaries are missed.\n2.  **Incomplete Event Representation:** Events are split, losing their full scope and context.\n3.  **Entity-Centric Bias:** Over-emphasis on local entity relationships rather than narrative flow.\n4.  **Limited Contextual Depth:** Insufficient context within chunks to establish narrative significance, resulting in numerous disconnected links.\n\n**Your Task:**\nAdvise on selecting a chunk size that maximizes narrative coherence and causal understanding in the extracted graph.\n\n**Reasoning Guidelines:**\nWhen evaluating potential chunk sizes, consider the following:\n\n*   **Narrative Flow & Causality:** Prioritize chunk sizes that are large enough to encompass significant narrative events, character actions, and their immediate causal links. Avoid sizes that frequently split core plot points or decision-action-consequence sequences. A larger chunk size can better preserve the context needed for the LLM to identify and link these narrative threads.\n*   **Event Completeness:** Assess if a chunk size allows for the holistic representation of events. Can an event, its key participants, and its immediate outcomes be captured within a single chunk?\n*   **Contextual Depth vs. Dilution:** A larger chunk size generally increases contextual depth, promoting a more narrative-centric graph. However, excessively large chunks might dilute specific information or exceed LLM context window limitations. Strive for a balance that provides sufficient narrative context without losing granularity.\n*   **Bias Mitigation:** Aim to move away from an entity-centric bias towards a more narrative-centric graph by ensuring chunks are large enough to contain meaningful narrative progression, not just isolated entity descriptions.\n\n**Output Requirements:**\nProvide a reasoned recommendation for a chunk size (or a range) for the given document. Justify your recommendation by explicitly referencing how it addresses the identified issues of fragmented causality, incomplete event representation, entity-centric bias, and limited contextual depth.\n\nBegin by analyzing the provided document's characteristics (e.g., density of narrative events, complexity of relationships) to inform your chunk size suggestion.",
  "learned_prompt_answer_generator_graph": "You are an AI assistant tasked with answering user queries using information retrieved from a graph knowledge base.\n\n**Your Goal:** Provide accurate, helpful, and coherent answers based *solely* on the provided `Information`.\n\n**Input:**\n*   **Query:** The user's question.\n*   **Information:** The relevant context retrieved from the graph.\n\n**Instructions:**\n\n1.  **Assess Sufficiency:** Critically evaluate if the provided `Information` directly and completely answers the `Query`.\n\n2.  **Sufficient Information:**\n    *   If the `Information` is sufficient, formulate a concise and accurate answer.\n    *   Ensure the answer is directly supported by the `Information`.\n    *   Do not introduce external knowledge or assumptions.\n\n3.  **Insufficient Information:**\n    *   If the `Information` is insufficient to answer the `Query`, clearly state this limitation.\n    *   **Explain *why*:** Briefly describe what specific information is missing or what aspects of the query cannot be addressed by the provided context (e.g., \"The provided information details character relationships but lacks specific plot events\").\n    *   **Guide the user:** Suggest the type of additional information that would be needed to answer their query.\n    *   **Offer alternatives:** If relevant and available in the `Information`, offer to provide related details that *can* be answered (e.g., \"I can provide a list of the characters involved,\" or \"I can tell you about their connections\").\n\n4.  **Formatting:** Present answers clearly and logically.\n\n**Prohibited Actions:**\n*   Do not invent or hallucinate information.\n*   Do not provide answers that are not directly derivable from the `Information`.\n*   Do not apologize excessively for limitations; be factual and constructive.\n\n**Query:**\n{}\n\n**Information:**\n{}",
  "learned_prompt_graph_retrieval_planner": "You are an AI assistant designed to plan retrieval operations for a GraphRAG system. Your goal is to construct a sequence of tool calls that effectively extracts information from a knowledge graph to answer user queries.\n\nWhen planning retrieval steps, prioritize understanding the **narrative and causal relationships** within the data, especially when the query relates to plot, events, or character interactions. Avoid plans that solely focus on identifying entities without exploring their connections.\n\n**Core Principles for Planning:**\n\n1.  **Focus on Relationships and Events:** The plot of a story is defined by how entities interact, their actions, motivations, and the causal links between events. Your retrieval plans must actively seek to uncover these connections. When exploring nodes, prioritize functions that retrieve *relationships* or *connected nodes based on specific relationship types*.\n2.  **Targeted Information Gathering:** Do not rely on generic searches. If a query is about a character's role in the plot, plan steps to find:\n    *   What actions they took (`get_node_by_relationship(\"Person\", \"performed_action\")`).\n    *   Who they interacted with and how (`get_node_by_relationship(\"Person\", \"interacted_with\", relationship_type=\"collaborates_with\")`, `get_node_by_relationship(\"Person\", \"interacted_with\", relationship_type=\"opposes\")`).\n    *   The consequences of their actions or events (`get_node_by_relationship(\"Event\", \"caused_by\")`, `get_node_by_relationship(\"Event\", \"led_to\")`).\n3.  **Narrative Reconstruction:** For plot-related queries, consider how to piece together a sequence of events. Functions like `analyze_path` can be crucial for understanding the progression between key entities or events.\n4.  **Iterative Refinement:** If initial steps yield limited relevant information (e.g., nodes with no explored connections), refine your plan to probe deeper into specific relationship types or explore alternative paths.\n5.  **Function Selection:** Choose functions that directly address the query's intent.\n    *   Use `search_nodes_by_types` primarily for initial entity identification or when relationship information is sparse.\n    *   Prioritize `get_node_by_relationship` and `get_nodes_by_relationships` to explore connections.\n    *   Utilize `analyze_path` to understand narrative flow.\n\n**Example:** If the query is \"What is the plot of the story?\" and you've identified the main character \"Alice,\" do not just search for more \"Person\" nodes. Instead, plan to find:\n*   `get_node_by_relationship(\"Alice\", \"performed_action\")`\n*   `get_node_by_relationship(\"Alice\", \"interacted_with\", relationship_type=\"met\")`\n*   `get_node_by_relationship(\"Alice\", \"interacted_with\", relationship_type=\"fought\")`\n*   `get_node_by_relationship(\"Alice\", \"involved_in\")`\n\nAlways consider the *meaning* of relationships in the context of the query. Your plan should reflect a strategy to build a coherent understanding of the narrative, not just a collection of isolated facts.",
  "learned_prompt_graph_builder": "You are an expert knowledge graph constructor for a GraphRAG system. Your primary goal is to build a narrative-rich graph that captures the causal and sequential flow of events in a story, enabling rich plot-focused queries.\n\nWhen processing the provided text, follow these instructions:\n\n1.  **Identify and Represent Key Entities:**\n    *   Extract significant entities (e.g., characters, major objects, locations, organizations) that play a role in the narrative.\n    *   For each entity, provide a *concise description* focusing on their relevance to the plot. Avoid exhaustive property extraction unless it directly impacts narrative causality or progression.\n\n2.  **Prioritize Plot-Driving Relationships:**\n    *   Focus on extracting relationships that explain *how* and *why* events unfold.\n    *   **Crucially, identify and label relationships such as:**\n        *   `causes`: Indicates a direct causal link between events or actions.\n        *   `leads to`: Shows a consequence or result.\n        *   `initiates event`: Marks the start of a significant occurrence.\n        *   `conflicts with`: Describes opposition or struggle between entities or events.\n        *   `sequences of`: Denotes events occurring in a specific order.\n        *   `precedes`: Explicitly states one event happens before another.\n        *   `consequence of`: Explicitly states one event is the result of another.\n        *   `involved in`: Links entities to events.\n    *   For each relationship, provide *brief, specific evidence* from the text that supports the connection.\n\n3.  **Represent Events as First-Class Nodes:**\n    *   Treat significant narrative events as distinct `Event` nodes.\n    *   Link `Event` nodes to the `Entity` nodes involved in them.\n    *   Connect `Event` nodes to each other using plot-driving temporal and causal relationships (e.g., `precedes`, `consequence of`, `causes`).\n\n4.  **Structure for Narrative Flow:**\n    *   Construct the graph with a narrative-centric approach. The sequence and causality of events should be the primary organizing principle, with entities described in relation to these events.\n    *   Avoid an entity-centric bias; the plot's progression is paramount.\n\n5.  **Output Format:**\n    *   Output the graph in JSON format, with distinct lists for `entities`, `events`, and `relationships`.\n    *   Ensure each node and relationship has a unique `id`.\n    *   Relationships should clearly reference the `source` and `target` node IDs and have a descriptive `label` (from the prioritized list above) and concise `evidence`.\n\nBy focusing on events and their causal/sequential connections, you will create a graph that accurately represents the narrative's progression, making it ideal for plot-driven analysis.",
  "hyperparameters_graph_agent_critique": "The chosen chunk size of 150 appears to be a contributing factor to the identified issues with the graph construction, specifically regarding narrative structure and causality.\n\n**Critique of Chunk Size (150):**\n\n1.  **Fragmented Narrative Causality:** A chunk size of 150, while not excessively small, can still lead to the fragmentation of causal chains and plot progression. If a significant event or a cause-and-effect relationship spans across two chunks, the LLM extracting the graph from each chunk might only capture a partial view. For instance, a character's decision in one chunk might lead to a consequence in the next. The relationship extraction LLM, working in isolation on each chunk, might fail to link these as cause and effect unless very specific prompts are used and the boundaries are carefully handled. This directly contributes to the \"lack of focus on narrative structure and causality\" identified in the critique.\n\n2.  **Incomplete Event Representation:** Events, especially those with broader implications, might be split. If an event is described across chunk boundaries, its full scope, participants, and consequences might not be captured cohesively within a single graph extraction. This leads to \"Weak Event Representation\" where individual events are not well-defined or connected to subsequent plot points. The LLM might extract isolated actions without understanding the preceding or succeeding narrative context that defines the event's significance.\n\n3.  **Entity-Centric Bias Amplified:** With a moderate chunk size, the focus can easily tilt towards extracting entities and their immediate local relationships *within* that chunk. This can reinforce the \"Entity-Centric vs. Narrative-Centric Approach.\" If a chunk primarily contains descriptions of characters and their surroundings without a clear action or plot driver, the extracted graph will reflect that. The system may struggle to identify overarching narrative actions that span multiple chunks.\n\n4.  **Over-reliance on Local Context:** Each chunk provides a limited local context. While 150 tokens can hold some information, it might not be enough to establish broader thematic connections or the importance of certain actions in the grander narrative. This can lead to the \"Limited Contextual Depth,\" where many relationships are extracted but lack the narrative weight to form a coherent plot. The sheer number of low-frequency relationships might be a symptom of this, where each small context yields a specific but ultimately disconnected link.\n\n**Recommendation:**\n\nConsider increasing the chunk size. A larger chunk size would provide more context for the LLM during graph extraction, potentially allowing it to:\n*   Capture more complete cause-and-effect relationships.\n*   Represent events and their immediate consequences more holistically.\n*   Identify narrative actions that span across larger textual segments, shifting from an entity-centric to a more narrative-centric view.\n\nThe optimal chunk size would likely require experimentation to balance capturing sufficient narrative context with avoiding overly large chunks that might dilute specific information or exceed LLM context window limitations.",
  "graph_builder_agent_critique": "This prompt for graph construction in GraphRAG exhibits several weaknesses that hinder the creation of a narrative-rich graph suitable for plot-focused queries.\n\n**1. Overemphasis on Granular Entity Properties vs. Narrative Relationships:**\nThe prompt instructs the LLM to extract detailed `properties` for each entity (e.g., \"age, role, description\"). While useful for general knowledge graphs, this level of detail often distracts from identifying the core narrative actions and causal links that drive a plot. The sheer volume of property extraction can dilute the focus on crucial relationships.\n\n**2. Lack of Explicit Instruction for Plot-Relevant Relationships:**\nThe prompt is too generic in its request for \"all the relationships.\" It doesn't guide the LLM to prioritize or identify relationships that are essential for understanding narrative progression. Relationships like `causes`, `leads to`, `initiates event`, `conflict with`, or `sequence of` are not prompted for, leading to a graph that is descriptive rather than dynamic.\n\n**3. Undervaluing Event Representation:**\nThe prompt doesn't specifically guide the LLM to represent events as distinct, connectable nodes. While entities are requested, events, which are the building blocks of a plot, are not emphasized as first-class citizens that can be linked to characters and other events via plot-driving relationships (e.g., `precedes`, `consequence of`).\n\n**4. Entity-Centric Bias:**\nThe prompt's structure naturally leads to an entity-centric graph. The focus is on identifying entities and then finding relationships *among* them. This contrasts with a narrative-centric approach where the *actions* and *sequences of events* involving entities are the primary focus, dictating the structure of the graph.\n\n**5. Ambiguity in Relationship Definition:**\nWhile the prompt asks for a `description` of why entities are related and `evidence`, the breadth of this request, combined with the lack of specific plot-oriented relationship types, results in the fragmentation observed in the generated graph (many relationship types with few instances). This suggests the LLM is inferring many tangential connections rather than focusing on the core narrative.\n\nIn summary, the prompt needs to be reframed to prioritize the extraction of causal and sequential relationships that define a plot, rather than focusing excessively on granular entity properties and a broad, unprioritized set of general relationships. The LLM should be instructed to identify and represent events and their connections more prominently.",
  "retrieval_planner_agent_critique": "## Critique of Retrieval Plans for \"What is the plot of the story?\"\n\nThe current retrieval plans are **fundamentally flawed** due to a consistent failure to extract any meaningful plot-related information. The core problem is the system's inability to move beyond identifying entities and their status as \"unexplored.\"\n\n**Major Issues:**\n\n1.  **Absence of Relationship/Event Extraction:** The repeated outcome of \"The connections of Node [Character Name] have not been explored yet\" clearly indicates that the graph generation and exploration phases are not capturing the narrative fabric of the story. The plot is defined by character interactions, conflicts, and causal links, none of which are being surfaced. The LLM's focus appears to be on entity identification rather than the relationships that form the plot.\n\n2.  **Ineffective Exploration Strategy:** The chosen retrieval actions (e.g., `search_nodes_by_types(\"Person\")`) are too generic. While identifying characters is a starting point, the plans do not incorporate steps to explore their *interactions*, *actions*, or *motivations*â€”elements crucial for understanding a plot. The agent needs to actively query for relationship types like \"attacked,\" \"collaborates with,\" \"betrays,\" etc., rather than passively waiting for exploration.\n\n3.  **Lack of Focus on Narrative Structure:** The query is about \"plot,\" which implies a sequence of events and their causal relationships. The current strategy does not leverage any functions that could help reconstruct this narrative, such as analyzing paths between key events or characters, or identifying pivotal actions.\n\n4.  **Inconsistent/Empty Graph Generation (Implied):** While not explicitly detailed in *every* prompt, the repeated \"connections not explored\" suggests that either the graphs are being generated without relationships or the exploration functions are not correctly retrieving them, leading to a barren graph structure. This inconsistency prevents any progress.\n\n**Recommendations for Improvement:**\n\n1.  **Prioritize Relationship and Event Extraction in Graph Generation:** The prompts for generating graphs from text chunks must explicitly instruct the LLM to extract *relationships* between entities, including actions, motivations, and events, not just the entities themselves.\n2.  **Develop Targeted Exploration Steps:** The retrieval plan should include steps to specifically search for relationship types relevant to plot development (e.g., \"attacks,\" \"converses with,\" \"agrees to help,\" \"antagonist\"). The agent needs to actively probe for these connections.\n3.  **Integrate Narrative Reconstruction Functions:** Consider utilizing functions that can help build a narrative. For example, once key characters or events are identified, using `analyze_path` between them could reveal plot progression.\n4.  **Refine Entity Understanding:** The system should aim to identify not just *who* is in the story but *what they do* and *how they relate to each other*. This requires focusing on the semantic meaning of relationships.",
  "answer_generation_critique": "The provided feedback correctly identifies the core issue: the generated answer is accurate but unhelpful. The critique and suggestions are well-reasoned, focusing on transforming a negative response (\"cannot do\") into a positive and constructive one.\n\nHere's a critique of the *current answer generation prompt* based on the example and feedback, with suggestions for improvement:\n\n**Critique of the Current Prompt:**\n\nThe current prompt is very basic and only instructs the LLM to answer the query using the provided information. It lacks any guidance on how to handle situations where the information is insufficient or irrelevant to the query.\n\n*   **Lack of Robustness for Insufficient Context:** The prompt doesn't anticipate or guide the LLM on what to do when the retrieved `information` does not contain the answer. It implicitly expects a direct answer to be present.\n*   **No Instruction on Handling Limitations:** There's no directive for the LLM to explain *why* it cannot answer, offer alternatives, or guide the user on how to improve their query or provide better context.\n*   **Missed Opportunity for User Guidance:** The prompt doesn't leverage the LLM's capabilities to help the user refine their input or understand the system's limitations.\n\n**Suggestions for Improving the Prompt:**\n\nTo address the issues highlighted in the feedback, the prompt needs to be more sophisticated and guide the LLM on conversational strategies when faced with incomplete information.\n\n1.  **Incorporate \"Handling Insufficient Information\" Directives:**\n    *   Explicitly instruct the LLM to assess if the provided `information` directly answers the `query`.\n    *   If the information is insufficient, the prompt should guide the LLM to:\n        *   **Clearly state the limitation:** Explain *what* information is missing (e.g., \"narrative details,\" \"plot points\").\n        *   **Explain *why* it's missing:** Refer back to the nature of the retrieved `information` (e.g., \"only contains character lists\").\n        *   **Offer alternative actions or available information:** Suggest what *can* be provided from the current context (e.g., \"I can list the characters\") or what the user could provide next.\n\n2.  **Enhance Guidance for User Interaction:**\n    *   Include instructions to politely guide the user on what kind of information would be helpful for answering their specific `query`.\n\n**Revised Prompt Example (incorporating these suggestions):**\n\n```\nYou will be given a query and the information retrieved from a graph.\nYour goal is to use the retrieved context to answer the query.\n\nThis is the query:\n{}\n\nThis is the information:\n{}\n\nCritically assess if the provided information is sufficient to directly answer the query.\n\nIf the information is sufficient:\n  Provide a concise and accurate answer to the query based *only* on the provided information.\n\nIf the information is insufficient:\n  1. State clearly that you cannot answer the query with the current information.\n  2. Briefly explain *why* the information is insufficient, referencing the type of data that is missing (e.g., plot details, events, relationships).\n  3. Suggest what kind of additional information would be needed to answer the query.\n  4. Optionally, offer to provide any relevant information that *is* available in the context, even if it doesn't fully answer the query.\n\nDo not invent information or provide answers not directly supported by the provided context.\n```\n\n**Rationale for Changes:**\n\n*   The `Critically assess` instruction forces the LLM to evaluate the context first.\n*   The `If the information is insufficient` block provides a structured approach to generating helpful responses, directly addressing the feedback's weaknesses.\n*   The mention of \"type of data that is missing\" and \"what kind of additional information would be needed\" guides the LLM to give specific, actionable feedback to the user.\n*   The optional offer to provide available data adds a layer of helpfulness, similar to Improvement Strategy 2 in the feedback.\n\nBy making these adjustments, the prompt will encourage the LLM to generate more informative and user-friendly responses when encountering limitations in the retrieved graph data.",
  "graph_builder_prompt": "\nYou will be given a text. Your goal is to identify entities in the text and all the relationships among the identified entities.\nFor each entity, you will include:\n- name: the entity name\n- type: the entity type (e.g., Person, Organization, Location, Event, Concept)\n- properties: a list of key-value pairs describing characteristics of the entity extracted from the text (e.g., for a person: age, role, description; for a location: description, significance). Each property should have a \"key\" and \"value\" field.\n\nFor each relationship, you will include its type, a description (why you think the two entities are related to each other), and the evidence from the text that supports this.\nThe relationships must be among the extracted entities.\nProvide a list of triplets in your answer.\n\nText:\n{TEXT_CHUNK}\n\nProvide the reasoning that led to your response.\n",
  "retrieval_prompt": "\nYour goal is to decide the next step of a strategy to explore a graph in order to retrieve relevant information to answer the following query: What is the plot of the story?.\n\nA high-level description of the graph is the following: This graph contains 125 nodes and 232 relationships. The graph density is 0.0299, indicating a sparsely connected network. The graph is fully connected with a fragmentation index of 0.0000. The entities consist of 45 \"Location\"s, 40 \"Person\"s, 14 \"Concept\"s, 14 \"Object\"s, 4 \"Organization\"s, 2 \"Structure\"s, 1 \"Armor\", 1 \"Creature\", 1 \"Event\", 1 \"Greeting\", 1 \"Plant\", and 1 \"Weapon\". The relationships include 7 \"attacked\" relationships, 5 \"associated with\" relationships, 5 \"uses\" relationships, 4 \"wields\" relationships, 3 \"accompanied by\" relationships, 3 \"affiliation\" relationships, 3 \"attacks\" relationships, 3 \"climbs\" relationships, 3 \"is\" relationships, 3 \"located in\" relationships, 3 \"possesses\" relationships, 2 \"antagonist\" relationships, 2 \"approached\" relationships, 2 \"approaches\" relationships, 2 \"are near\" relationships, 2 \"belongs to\" relationships, 2 \"citizen of\" relationships, 2 \"friend\" relationships, 2 \"guards\" relationships, 2 \"has image of\" relationships, 2 \"interacted with\" relationships, 2 \"killed\" relationships, 2 \"located near\" relationships, 2 \"located on\" relationships, 2 \"observes\" relationships, 2 \"trapped on\" relationships, 2 \"traveled to\" relationships, 2 \"wears\" relationships, 1 \"agrees to help\" relationship, 1 \"attacked\" relationship, 1 \"awareness of location\" relationship, 1 \"belief about\" relationship, 1 \"converses with\" relationship, 1 \"calls\" relationship, 1 \"comparison of appearance\" relationship, 1 \"dwell in\" relationship, 1 \"expresses intent to live with\" relationship, 1 \"from\" relationship, 1 \"feared\" relationship, 1 \"from\" relationship, 1 \"gave\" relationship, 1 \"has relationship with\" relationship, 1 \"interacted with\" relationship, 1 \"is related to\" relationship, 1 \"is sibling of\" relationship, 1 \"identification\" relationship, 1 \"identifies\" relationship, 1 \"informed\" relationship, 1 \"instruction\" relationship, 1 \"interacted with\" relationship, 1 \"is\" relationship, 1 \"killed\" relationship, 1 \"lived in\" relationship, 1 \"mentioned as destination\" relationship, 1 \"motive for helping\" relationship, 1 \"observation\" relationship, 1 \"origin of people\" relationship, 1 \"perception/judgment\" relationship, 1 \"protection\" relationship, 1 \"proximity/shared location\" relationship, 1 \"relates to\" relationship, 1 \"requests help from\" relationship, 1 \"searched for\" relationship, 1 \"seeks to rescue\" relationship, 1 \"traveled from\" relationship, 1 \"travel towards\" relationship, 1 \"visited\" relationship, 1 \"wants information about\" relationship, 1 \"accused by\" relationship, 1 \"acquaintance\" relationship, 1 \"addresses\" relationship, 1 \"admires\" relationship, 1 \"antagonized\" relationship, 1 \"are at\" relationship, 1 \"asked about\" relationship, 1 \"assists\" relationship, 1 \"belonged to\" relationship, 1 \"called\" relationship, 1 \"captured\" relationship, 1 \"carries\" relationship, 1 \"collided with\" relationship, 1 \"combat\" relationship, 1 \"commander/leader\" relationship, 1 \"compared to\" relationship, 1 \"connected to\" relationship, 1 \"contains\" relationship, 1 \"covet\" relationship, 1 \"crashed on\" relationship, 1 \"disagrees with\" relationship, 1 \"dons\" relationship, 1 \"dropped\" relationship, 1 \"encounters\" relationship, 1 \"enemy\" relationship, 1 \"enemy of\" relationship, 1 \"entered\" relationship, 1 \"enters\" relationship, 1 \"exile from\" relationship, 1 \"faces\" relationship, 1 \"familial\" relationship, 1 \"fell down\" relationship, 1 \"fought against\" relationship, 1 \"from\" relationship, 1 \"goes to\" relationship, 1 \"greeted\" relationship, 1 \"guarded\" relationship, 1 \"held in\" relationship, 1 \"helps ascend\" relationship, 1 \"hides bodies in\" relationship, 1 \"hunter\" relationship, 1 \"identifies\" relationship, 1 \"illuminates\" relationship, 1 \"inhabitant of\" relationship, 1 \"instructed\" relationship, 1 \"intends to rescue\" relationship, 1 \"interacted with\" relationship, 1 \"interacts with\" relationship, 1 \"interrogated\" relationship, 1 \"is a satellite of\" relationship, 1 \"is associated with\" relationship, 1 \"is in\" relationship, 1 \"is inserted into\" relationship, 1 \"is interior of\" relationship, 1 \"is locked by\" relationship, 1 \"is part of\" relationship, 1 \"is secured by\" relationship, 1 \"is served by\" relationship, 1 \"is worshipped as\" relationship, 1 \"kills\" relationship, 1 \"landed on\" relationship, 1 \"lead to\" relationship, 1 \"leader of\" relationship, 1 \"leads to\" relationship, 1 \"located in\" relationship, 1 \"mate\" relationship, 1 \"named\" relationship, 1 \"named after\" relationship, 1 \"observing\" relationship, 1 \"obstructed\" relationship, 1 \"orbits\" relationship, 1 \"originates from\" relationship, 1 \"part of\" relationship, 1 \"perceived as\" relationship, 1 \"prefers\" relationship, 1 \"prefers less than\" relationship, 1 \"prepared to battle\" relationship, 1 \"prevented\" relationship, 1 \"protected\" relationship, 1 \"provided comfort\" relationship, 1 \"pursued\" relationship, 1 \"pursued by\" relationship, 1 \"reaches\" relationship, 1 \"received message from\" relationship, 1 \"recognized\" relationship, 1 \"religious\" relationship, 1 \"requests support from\" relationship, 1 \"rescued\" relationship, 1 \"restored\" relationship, 1 \"returned to\" relationship, 1 \"sacrifices to\" relationship, 1 \"said\" relationship, 1 \"searched for\" relationship, 1 \"seeks secret of\" relationship, 1 \"sibling\" relationship, 1 \"speaks to\" relationship, 1 \"strangles\" relationship, 1 \"struck\" relationship, 1 \"subordinate to\" relationship, 1 \"taken from beside\" relationship, 1 \"threatened\" relationship, 1 \"took\" relationship, 1 \"trapped with\" relationship, 1 \"traveled from\" relationship, 1 \"traveled in\" relationship, 1 \"travels to\" relationship, 1 \"traversed\" relationship, 1 \"used\" relationship, 1 \"wants to save\" relationship, 1 \"wants to conquer\" relationship, 1 \"wants to make invincible\" relationship, 1 \"watched\" relationship, 1 \"worked in\" relationship, 1 \"working with\" relationship, 1 \"worn by\" relationship, and 1 \"worshipped by\" relationship. The most common entity type is \"Location\" with 45 instances. The most frequent relationship type is \"attacked\" with 7 occurrences.\n\nYou must choose one of the following functions:\n\n- search_nodes_by_keyword(keyword): search for all the nodes whose labels contain the given keyword\n- search_nodes_by_types(node_type): search for all the nodes whose type property contains the given type\n- get_neighbors(node_name): get all neighbors of a node with the given name\n- search_relations_by_type(relation_type): search for all the triplets whose relationship matches the type\n- identify_communities(node_name): find the community (connected component) containing a specific node\n- analyze_path(start_node_name, end_node_name): find the shortest path between two nodes\n- find_hub_nodes: find the top 3 hub nodes with the highest connectivity\n\nThe subgraphs you retrieved so far are the following:\n\n{RETRIEVED_CONTEXT}\n\nChoose one of the functions and specify the arguments.\n\nProvide the reasoning that led to your response.\n\nPay attention to symbols included in the entity/relationship type names: make sure to include them in your search for matching to succeed.\nAlso, pay attention to symbols included in the functions names. The name of the function called must exactly match one of the functions above. \n"
}