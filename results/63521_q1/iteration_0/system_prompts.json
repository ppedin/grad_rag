{
  "learned_prompt_hyperparameters_graph": "The current chunk size of 256 tokens is suboptimal for GraphRAG due to:\n\n1.  **Narrative Fragmentation:** This size is too small to encompass complete events or sequential actions, splitting crucial cause-and-effect chains. For instance, an action and its immediate consequence may reside in separate chunks, weakening their causal link.\n\n2.  **Weak Narrative Threads:** Sequential information is difficult to preserve. If a chunk contains only the setup for an event, the subsequent chunk containing the resolution will be processed with diminished context, leading to isolated facts instead of a flowing narrative.\n\n3.  **Over-reliance on Static Relationships:** Limited context favors static relationships (e.g., \"is located in\") over dynamic, plot-driving ones (e.g., \"attacks,\" \"discovers\") which require understanding event sequences.\n\n4.  **Increased Noise:** Granular extraction from small chunks can introduce irrelevant details without crucial context, cluttering the graph with weak, uninformative connections.\n\n**Recommendation:** Increase chunk size (e.g., to 512 or 1024 tokens) to enable the capture of more complete narrative units. This will improve the extraction of sequential, causal information, leading to a more coherent and plot-driven graph representation.",
  "learned_prompt_answer_generator_graph": "You are an AI assistant designed to answer user queries using information retrieved from a knowledge graph. Your responses should be helpful, comprehensive, and user-centric, especially when the exact information is not directly available.\n\n**Your Goal:** To effectively utilize the provided retrieved information to answer the user's query.\n\n**Input:**\n*   **Query:** The user's question.\n*   **Retrieved Information:** Contextual data extracted from the knowledge graph relevant to the query.\n\n**Instructions for Generating Your Answer:**\n\n1.  **Direct Answer Fulfillment:**\n    *   If the `Retrieved Information` *directly and completely* answers the `Query`, provide a clear, concise, and accurate answer based solely on the provided context. Cite the information source if applicable and helpful.\n\n2.  **Handling Incomplete or Missing Information:**\n    *   If the `Retrieved Information` *does not directly or completely* answer the `Query`:\n        *   **Acknowledge the Query:** Begin by acknowledging that you understand the user's specific question.\n        *   **Identify and State the Information Gap:** Clearly state that the precise information requested is not present in the provided context. Be specific about *what* is missing (e.g., \"The provided information does not contain a summary of the plot.\").\n        *   **Summarize Available, Related Information:** Briefly describe the *type* of information that *is* available in the `Retrieved Information`. Focus on aspects that are thematically related to the query (e.g., \"However, I have details about the main characters, their relationships, and the primary settings.\").\n        *   **Propose Next Steps or Offer Alternatives:**\n            *   Suggest providing the related information that you *do* have (e.g., \"Would you be interested in learning about the main characters' backgrounds?\").\n            *   Ask clarifying questions to help the user refine their query or explore related avenues (e.g., \"Could you clarify if you're interested in character motivations or specific events?\").\n            *   If applicable, mention related entities or concepts found in the context that might be of interest (e.g., \"The context mentions the director of the film, perhaps you'd like to know more about them?\").\n\n3.  **User-Centric Principles:**\n    *   **Prioritize Helpfulness:** Always aim to provide the most value to the user, even when the direct answer is unavailable. Avoid abrupt or unhelpful responses that simply state a lack of information.\n    *   **Maintain Coherence:** Ensure your response flows logically and is easy to understand.\n    *   **Stay Within Context:** Base your entire answer on the `Retrieved Information`. Do not introduce external knowledge. If information is truly absent, state that clearly.\n\n**Example Scenario (for understanding, not part of the prompt):**\n\n*   **Query:** \"What is the plot summary of the movie 'Inception'?\"\n*   **Retrieved Information:** (Contains details about actors, director, release date, genres, main characters' names and roles, but no plot summary.)\n\n*   **Undesired Response:** \"I cannot answer this question.\"\n*   **Desired Response:** \"I understand you're asking for a plot summary of the movie 'Inception.' Unfortunately, the provided information does not contain a summary of the plot. However, I do have details about the main characters, such as Cobb and Mal, their relationships, and the director, Christopher Nolan. Would you be interested in learning more about the characters or the director?\"",
  "learned_prompt_graph_retrieval_planner": "You are a sophisticated AI agent designed to retrieve and synthesize information from a knowledge graph to answer user queries. Your primary function is to construct narrative plots from the graph, not just collect static facts.\n\n**Core Objective:** Extract and assemble a coherent plot from the knowledge graph. This means identifying a sequence of events, actions, and their causal relationships that form a narrative arc, rather than simply gathering information about entities.\n\n**Key Principles for Retrieval Planning:**\n\n1.  **Prioritize Event and Action Extraction:**\n    *   Focus on identifying *what happens*. Look for nodes and relationships that represent actions, events, and occurrences.\n    *   Extract verb-centric information: identify subjects, verbs, objects, and any associated temporal or causal modifiers.\n    *   Recognize relationships that signify causality, consequence, or temporal progression (e.g., \"leads to,\" \"causes,\" \"after,\" \"during\").\n\n2.  **Uncover Narrative Progression and Causality:**\n    *   Plan retrieval strategies that reveal the *sequence* of events. How does one event lead to another? What are the turning points?\n    *   Identify chains of causality. Understanding *why* things happen is crucial for plot construction.\n    *   Avoid solely exploring static entity attributes or simple associations. These are building blocks, not the plot itself.\n\n3.  **Adopt Narrative Traversal Strategies:**\n    *   When exploring the graph, aim to follow threads of action and consequence over time.\n    *   Utilize functions like `analyze_path` or `get_neighbors` with a specific goal of uncovering sequential events or causal links, not just connectivity.\n    *   If the graph supports it, explore temporal or causal relationship types.\n\n4.  **Synthesize Information into a Plot:**\n    *   Your final output should be a narrative, not a list of facts.\n    *   The retrieval plan should be geared towards gathering the necessary components (events, characters' actions, consequences) to construct a story with a beginning, middle, and end.\n    *   Consider conflict, resolution, and character development as integral parts of the plot.\n\n**Function Usage Guidelines:**\n\n*   **`get_neighbors(entity_id, relationship_type)`:** Use to find entities directly connected to another. When planning, specify `relationship_type` to target action, event, or causal links if possible, rather than just general associations.\n*   **`analyze_path(entity_id1, entity_id2)`:** Use to understand the connection between two entities. For plot generation, aim to analyze paths that represent a sequence of events or a causal chain.\n*   **`get_entity_details(entity_id)`:** Use to understand the nature of an entity, particularly its role in events or actions.\n*   **`search_entities(query)`:** Use to find key entities or events that are central to a potential plot.\n\nYour retrieval plans should demonstrate a clear strategy for moving from static entities to dynamic events and then assembling these into a narrative plot. Always ask yourself: \"How does this piece of information contribute to the *story* or the *sequence of events*?\"",
  "learned_prompt_graph_refinement": "",
  "hyperparameters_graph_agent_critique": "The chunk size of 256 tokens, while seemingly arbitrary, significantly impacts the narrative coherence of the extracted graph. The primary issue stemming from this chunk size is the **fragmentation of plot elements and the resulting difficulty in establishing narrative causality**.\n\nHere's a detailed critique:\n\n1.  **Inability to Capture Sequential Actions within a Chunk:** A chunk size of 256 tokens is likely too small to contain a complete, self-contained event or a series of closely related actions that drive the plot forward. For instance, a sequence like \"character A plans to do X, then travels to location Y, and then confronts character B\" might be split across multiple chunks. This means a single, coherent action-reaction sequence within the narrative is broken down, making it hard to extract as a unified event or relationship.\n\n2.  **Weak Narrative Threads Across Chunks:** The current graph description highlights a lack of sequential and causal information. A 256-token chunk size exacerbates this. If a chunk only contains a fragment of an action or a setup for an event, the subsequent chunk will contain the resolution. When these are processed independently for graph extraction, the crucial link (the transition from setup to resolution) is lost or weakened. This leads to isolated facts rather than a flowing narrative.\n\n3.  **Over-emphasis on Static Relationships due to Limited Context:** With smaller chunks, the LLM tasked with extracting graphs might default to identifying more static, descriptive relationships (like \"associated with,\" \"located in,\" \"friend\") because these are more easily identifiable within limited context. Dynamic, plot-driving relationships (e.g., \"attacks,\" \"conspires,\" \"discovers\") that require understanding a sequence of events are harder to capture accurately when the context is restricted to 256 tokens.\n\n4.  **Increased Noise and Irrelevant Information:** Conversely, a very small chunk size might also lead to overly granular and less meaningful extracted information. A sentence might contain a character's name and a location, but without the surrounding sentences, the *purpose* of their being there or their interaction is lost. This can clutter the graph with many weak connections that don't contribute to understanding the main plot.\n\n**Recommendation for Improvement:**\n\nThe chunk size should be adjusted to allow for the capture of more complete narrative units. This means a chunk should ideally be large enough to contain:\n*   A single, significant event and its immediate participants.\n*   A sequence of closely related actions that form a cause-and-effect chain.\n*   Sufficient context to disambiguate relationships and understand the intent behind actions.\n\nExperimenting with larger chunk sizes (e.g., 512 or 1024 tokens) would likely improve the extraction of sequential and causal information, leading to a more coherent and plot-driven graph. This would better align with the critique's recommendations for prioritizing event and action extraction and developing narrative sequencing mechanisms.",
  "graph_builder_agent_critique": "Here's a critique of the graph construction prompt, focusing on its limitations for representing plot and narrative flow, and suggesting improvements:\n\n**Critique of the Graph Construction Prompt:**\n\nThe current prompt is effective at extracting entities and their static relationships, leading to a rich knowledge base about the text's components. However, it falls short in capturing the dynamic, sequential, and causal nature of a narrative plot.\n\n1.  **Overemphasis on Static Attributes:** The prompt's structure, asking for entity `name`, `type`, and `properties`, and then relationships with `type`, `description`, and `evidence`, heavily favors factual extraction of states and affiliations. This leads to a graph dense with relationships like \"associated with,\" \"inhabitant of,\" and \"friend,\" which describe *what is* rather than *what happens*. Plots are driven by actions and events, not static descriptions.\n\n2.  **Lack of Temporal and Causal Information:** The prompt does not instruct the LLM to identify or represent the *order* of events or their causal links. Relationships are presented as independent facts. This makes it impossible to reconstruct a narrative sequence, identify rising action, climax, or resolution, or understand cause-and-effect chains crucial for plot comprehension.\n\n3.  **Insufficient Focus on Action and Event Extraction:** While some action-oriented relationship types might emerge (e.g., \"attacks\"), the prompt doesn't prioritize them. There's no explicit instruction to identify verbs as core plot drivers or to extract the temporal and sequential aspects of actions. The \"description\" field for relationships is too general to capture plot progression.\n\n4.  **Limited Narrative Structure:** The prompt treats the text as a collection of facts to be turned into nodes and edges. It doesn't encourage the extraction of narrative structures like conflicts, goals, obstacles, or outcomes, which are fundamental to storytelling. The reasoning provided by the LLM will likely focus on justifying entity/property/relationship extraction rather than narrative coherence.\n\n**Recommendations for Prompt Improvement:**\n\n1.  **Explicitly Instruct Event and Action Extraction:** Modify the prompt to prioritize identifying and representing actions and events. Instruct the LLM to look for verbs that signify plot progression and to extract them as relationships with clear participants and, if possible, temporal or sequential context.\n    *   *Example Instruction:* \"Prioritize extracting actions and events. For each significant action, identify the actor, the action (as the relationship type), and the patient/object. If the text provides temporal cues (e.g., 'then', 'afterward', 'later'), capture this sequence.\"\n\n2.  **Incorporate Temporal/Sequential Attributes:** Introduce fields to capture the order or timing of events.\n    *   *Example Instruction:* \"For relationships representing actions or events, include an optional 'sequence_order' field if inferable from the text, or a 'temporal_cue' field (e.g., 'early', 'later', 'simultaneously') if explicitly mentioned.\"\n\n3.  **Encourage Plot-Relevant Relationship Types:** Guide the LLM to identify relationships that are critical to plot development (e.g., conflict, motivation, intention, resolution).\n    *   *Example Instruction:* \"Focus on relationships that drive the narrative forward, such as 'plans to attack', 'attempts to escape', 'discovers a secret', 'conflicts with', 'resolves issue with'.\"\n\n4.  **Request Narrative Reasoning:** Modify the \"Provide the reasoning\" section to specifically ask for how the extracted elements contribute to the plot's narrative flow.\n    *   *Example Instruction:* \"Provide the reasoning that led to your response, explaining how the extracted entities and relationships contribute to the unfolding plot and sequence of events.\"\n\nBy adjusting the prompt to explicitly solicit temporal, causal, and action-oriented information, the graph construction process can shift from creating a static knowledge graph to a more dynamic representation of the story's plot.",
  "retrieval_planner_agent_critique": "The current retrieval plans demonstrate a fundamental misunderstanding of how to extract a \"plot\" from a graph. The system excels at identifying entities and their static relationships but fails to capture the dynamic, event-driven nature of a narrative.\n\n**Critique:**\n\n1.  **Lack of Narrative Progression:** The core issue is the absence of a strategy to reconstruct the sequence of events. The plans focus on identifying entities (e.g., \"Person\" nodes) and their immediate connections (e.g., \"get_neighbors\"). This leads to a collection of disconnected facts rather than a coherent story arc. The extracted relationships like \"associated with,\" \"from,\" and \"interacted with\" are too static to build a plot.\n\n2.  **Failure to Prioritize Action and Causality:** A plot is defined by *what happens* and *why*. The current approach does not prioritize or even attempt to extract actions, events, or causal links between them. For instance, knowing \"Captain Dietrich is from America\" is a factual attribute, not a plot point. The system needs to extract verbs and their subjects/objects that represent actions and consequences.\n\n3.  **Over-reliance on Entity-Centric Exploration:** While identifying key entities is a necessary first step, subsequent exploration must pivot towards understanding their actions and interactions *over time*. Repeatedly calling `get_neighbors` on central characters without a clear objective to find sequential events or causal chains will not reveal the plot.\n\n4.  **Inability to Synthesize:** The retrieved context, even if it contained more event-like information, would likely be presented as fragmented pieces. The agent needs capabilities to synthesize these fragments into a narrative, identifying the beginning, rising action, climax, falling action, and resolution. This is not achieved by simply gathering more static relationships.\n\n**Recommendations for Improvement:**\n\n1.  **Shift Focus to Event Extraction:** The graph extraction process must be adapted to identify and represent events and actions. This involves extracting verb-centric information and their participants, creating relationships that signify \"Action X performed by Y on Z\" or \"Event E leads to Event F.\"\n2.  **Develop Narrative Traversal Strategies:** Retrieval plans should aim to uncover sequences of events. This might involve identifying temporal relationships, chain reactions, or following threads of causality. Functions like `analyze_path` could be more useful if paths represent sequences of events rather than just shortest distances between entities.\n3.  **Prompt for Narrative Synthesis:** The LLM agent needs to be prompted to look for narrative progression, conflict, resolution, and character development. The goal should not just be to collect information but to assemble it into a story.\n4.  **Refine Graph Schema:** Consider adding relationship types that explicitly denote temporal order or causality, if possible, to better represent narrative flow.",
  "answer_generation_critique": "Here's a critique of the previous answer generation prompt and suggestions for improvement, focusing on the identified issues:\n\n**Critique of the Current Prompt:**\n\nThe current prompt is very basic and lacks specific instructions for handling situations where the retrieved information doesn't directly answer the query. It simply asks the LLM to use the provided information to answer the query. This leads to:\n\n1.  **Generic and Unhelpful Responses:** As seen in the example, the LLM correctly identifies the missing information but offers no further assistance. The prompt doesn't instruct the LLM to be more proactive or helpful.\n2.  **Lack of Contextualization:** The prompt doesn't guide the LLM to explain *what* information *is* available, leaving the user with a dead end.\n3.  **No Guidance on User Intent:** The prompt doesn't encourage the LLM to infer user intent or suggest alternative ways to help.\n\n**Proposed Prompt Improvements:**\n\nTo address these weaknesses, the prompt needs to be more directive, guiding the LLM on how to formulate a comprehensive and helpful response, especially when the direct answer isn't found.\n\n**Revised Prompt Suggestion:**\n\n```\nYou will be given a query and the information retrieved from a knowledge graph. Your goal is to use the retrieved context to answer the query in a helpful and comprehensive manner.\n\n**Query:**\n{}\n\n**Retrieved Information:**\n{}\n\n**Instructions for Answering:**\n\n1.  **Direct Answer:** If the retrieved information directly answers the query, provide a clear and concise answer.\n2.  **Information Gaps:** If the retrieved information *does not* directly answer the query:\n    *   **Acknowledge the query:** State that you understand what the user is asking for.\n    *   **Explain what information is missing:** Clearly state that the specific information requested (e.g., plot summary) is not present in the provided context.\n    *   **Describe the available information:** Briefly summarize the *type* of information that *is* present in the retrieved context (e.g., character relationships, locations, entities, etc.).\n    *   **Offer to provide related information:** Suggest alternative information that *is* available and might be useful to the user, or ask clarifying questions to help refine the search. For example, \"Would you be interested in learning about the relationships between characters X and Y?\" or \"I found information on the locations mentioned, would that be helpful?\"\n3.  **Be User-Centric:** Aim to guide the user and provide the most value possible, even if the exact query cannot be fulfilled. Avoid responses that simply state a lack of information without offering further assistance.\n```\n\n**Reasoning for Changes:**\n\n*   **Explicit Instructions for Gaps:** Section 2.a-d specifically instructs the LLM on how to handle missing information, directly addressing the feedback from the example.\n*   **\"Describe Available Information\":** This encourages the LLM to provide context about what *is* there, making the response more informative.\n*   **\"Offer to Provide Related Information/Clarifying Questions\":** This promotes a more interactive and helpful experience, guiding the user towards potentially useful data or helping them refine their query.\n*   **\"User-Centric\" Principle:** This overarching instruction reinforces the goal of providing value beyond a simple factual retrieval.\n\nThis revised prompt should lead to more robust and helpful responses from the LLM, transforming unhelpful \"dead ends\" into opportunities for further user engagement and information discovery.",
  "graph_builder_prompt": "\nYou will be given a text. Your goal is to identify entities in the text and all the relationships among the identified entities.\nFor each entity, you will include:\n- name: the entity name\n- type: the entity type (e.g., Person, Organization, Location, Event, Concept)\n- properties: a list of key-value pairs describing characteristics of the entity extracted from the text (e.g., for a person: age, role, description; for a location: description, significance). Each property should have a \"key\" and \"value\" field.\n\nFor each relationship, you will include its type, a description (why you think the two entities are related to each other), and the evidence from the text that supports this.\nThe relationships must be among the extracted entities.\nProvide a list of triplets in your answer.\n\nText:\n{TEXT_CHUNK}\n\nProvide the reasoning that led to your response.\n",
  "retrieval_prompt": "\nYour goal is to decide the next step of a strategy to explore a graph in order to retrieve relevant information to answer the following query: What is the plot of the story?.\n\nA high-level description of the graph is the following: This graph contains 94 nodes and 194 relationships. The graph density is 0.0444, indicating a sparsely connected network. The graph is fully connected with a fragmentation index of 0.0000. The entities consist of 38 \"Location\"s, 32 \"Person\"s, 7 \"Concept\"s, 4 \"Group\"s, 4 \"Vehicle\"s, 3 \"Deity\"s, 2 \"Organization\"s, 1 \"Animal\", 1 \"Event\", 1 \"Object\", and 1 \"Weapon\". The relationships include 5 \"associated with\" relationships, 4 \"attacks\" relationships, 4 \"inhabitant of\" relationships, 3 \"attacked by\" relationships, 3 \"captures\" relationships, 3 \"from\" relationships, 3 \"located in\" relationships, 2 \"associated with\" relationships, 2 \"enemy\" relationships, 2 \"friend\" relationships, 2 \"interacted with\" relationships, 2 \"learned language\" relationships, 2 \"located on\" relationships, 2 \"origin\" relationships, 2 \"describes\" relationships, 2 \"enters\" relationships, 2 \"escapes with\" relationships, 2 \"guards\" relationships, 2 \"interacts with\" relationships, 2 \"is\" relationships, 2 \"observes\" relationships, 2 \"on\" relationships, 2 \"possesses\" relationships, 2 \"trapped on\" relationships, 2 \"traveled to\" relationships, 2 \"traveling with\" relationships, 2 \"travels towards\" relationships, 2 \"worship\" relationships, 1 \"adheres to\" relationship, 1 \"advocates for\" relationship, 1 \"approaches\" relationship, 1 \"associate\" relationship, 1 \"attack\" relationship, 1 \"attacked\" relationship, 1 \"calls out to\" relationship, 1 \"chief of\" relationship, 1 \"commands\" relationship, 1 \"contains\" relationship, 1 \"delivers message to\" relationship, 1 \"dwelled with\" relationship, 1 \"exile from\" relationship, 1 \"from\" relationship, 1 \"goes to rescue\" relationship, 1 \"has\" relationship, 1 \"hides bodies near\" relationship, 1 \"identified as\" relationship, 1 \"identifies with\" relationship, 1 \"incapacitates\" relationship, 1 \"injured by\" relationship, 1 \"killed\" relationship, 1 \"leader of\" relationship, 1 \"leads to\" relationship, 1 \"named by\" relationship, 1 \"opens into\" relationship, 1 \"orbits\" relationship, 1 \"perceives\" relationship, 1 \"referred to as\" relationship, 1 \"related to\" relationship, 1 \"resides in\" relationship, 1 \"speaks to\" relationship, 1 \"satellite of\" relationship, 1 \"travels to\" relationship, 1 \"uses\" relationship, 1 \"accompanies\" relationship, 1 \"accused\" relationship, 1 \"addressed\" relationship, 1 \"affiliated with\" relationship, 1 \"among\" relationship, 1 \"ancestors from\" relationship, 1 \"approaches\" relationship, 1 \"ascends/descends\" relationship, 1 \"assists\" relationship, 1 \"assures\" relationship, 1 \"attacked\" relationship, 1 \"belonged to\" relationship, 1 \"blocks\" relationship, 1 \"born on\" relationship, 1 \"built\" relationship, 1 \"can be trapped and skinned\" relationship, 1 \"carries\" relationship, 1 \"chosen for\" relationship, 1 \"claims to not be\" relationship, 1 \"combat\" relationship, 1 \"confronts\" relationship, 1 \"considers beautiful\" relationship, 1 \"daughter of\" relationship, 1 \"descends toward\" relationship, 1 \"desires\" relationship, 1 \"dwells in\" relationship, 1 \"encountered\" relationship, 1 \"enemies\" relationship, 1 \"eternal war\" relationship, 1 \"examines\" relationship, 1 \"falls down\" relationship, 1 \"father of\" relationship, 1 \"father of woman of\" relationship, 1 \"friend\" relationship, 1 \"gifted\" relationship, 1 \"guard\" relationship, 1 \"hunts near\" relationship, 1 \"ignored by\" relationship, 1 \"instilled fear in\" relationship, 1 \"instructs to\" relationship, 1 \"intends to rescue\" relationship, 1 \"intends to rescue\" relationship, 1 \"intends to take\" relationship, 1 \"interacted with\" relationship, 1 \"interrogated\" relationship, 1 \"is in\" relationship, 1 \"is part of\" relationship, 1 \"killed\" relationship, 1 \"kills\" relationship, 1 \"landed in\" relationship, 1 \"landed on\" relationship, 1 \"lives in\" relationship, 1 \"located beyond\" relationship, 1 \"mate of\" relationship, 1 \"mentioned\" relationship, 1 \"might attack\" relationship, 1 \"mingles with\" relationship, 1 \"originated from\" relationship, 1 \"originates from\" relationship, 1 \"plans to conquer\" relationship, 1 \"plans to make invincible\" relationship, 1 \"potential sacrifice to\" relationship, 1 \"potential threat to\" relationship, 1 \"prepares to battle\" relationship, 1 \"pulled by\" relationship, 1 \"pursued\" relationship, 1 \"pursues\" relationship, 1 \"recognizes\" relationship, 1 \"related to\" relationship, 1 \"rescues\" relationship, 1 \"resides in\" relationship, 1 \"restricted from\" relationship, 1 \"revises understanding of\" relationship, 1 \"rode\" relationship, 1 \"seeks secret of\" relationship, 1 \"sees\" relationship, 1 \"serves\" relationship, 1 \"speaks to\" relationship, 1 \"stripped\" relationship, 1 \"threatened\" relationship, 1 \"through\" relationship, 1 \"traveled from\" relationship, 1 \"travels to\" relationship, 1 \"used\" relationship, 1 \"uses\" relationship, 1 \"uses skin of\" relationship, 1 \"vanishes from\" relationship, 1 \"wants to escape from\" relationship, 1 \"warned about\" relationship, 1 \"was on\" relationship, 1 \"works to restore\" relationship, and 1 \"worshipper of\" relationship. The most common entity type is \"Location\" with 38 instances. The most frequent relationship type is \"associated with\" with 5 occurrences.\n\nYou must choose one of the following functions:\n\n- search_nodes_by_keyword(keyword): search for all the nodes whose labels contain the given keyword\n- search_nodes_by_types(node_type): search for all the nodes whose type property contains the given type\n- get_neighbors(node_name): get all neighbors of a node with the given name\n- search_relations_by_type(relation_type): search for all the triplets whose relationship matches the type\n- identify_communities(node_name): find the community (connected component) containing a specific node\n- analyze_path(start_node_name, end_node_name): find the shortest path between two nodes\n- find_hub_nodes: find the top 3 hub nodes with the highest connectivity\n\nThe subgraphs you retrieved so far are the following:\n\n{RETRIEVED_CONTEXT}\n\nChoose one of the functions and specify the arguments.\n\nProvide the reasoning that led to your response.\n\nPay attention to symbols included in the entity/relationship type names: make sure to include them in your search for matching to succeed.\nAlso, pay attention to symbols included in the functions names. The name of the function called must exactly match one of the functions above. \n"
}