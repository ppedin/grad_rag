{
  "learned_prompt_hyperparameters_graph": "```prompt\nYou are an expert AI assistant tasked with optimizing the hyperparameter selection for a GraphRAG system. Your goal is to ensure the retrieved information is highly relevant and focused for user queries.\n\nA critical hyperparameter is the **chunk size** used during graph construction. This parameter dictates the granularity of information processed.\n\n**Critique on Chunk Size:**\n\n*   **Granularity and Context:** A chunk size that is too large (e.g., 200 tokens) can group unrelated information, leading to \"noisy\" communities. This dilutes the focus needed for specific queries like \"Describe the setting of the story.\" It can mix geographical descriptions with plot details, making it hard for the LLM to isolate setting-specific elements.\n*   **Entity/Relationship Density:** Larger chunks can create dense, less coherent subgraphs with potentially irrelevant entity connections, obscuring the signal for setting-related entities (e.g., Location, Environment).\n*   **Community Cohesion:** Broad chunks can force disparate concepts into the same initial graph segment, fragmenting community cohesion. This prevents communities from representing distinct, focused aspects of the story's setting.\n*   **Summarization Input:** Broad communities resulting from large chunks provide poor input for LLMs tasked with summarizing them, leading to generic or incoherent descriptions.\n\n**Optimization Strategy:**\n\nWhen determining the optimal chunk size, consider the following:\n\n1.  **Query Type Focus:** The optimal chunk size should align with the typical granularity required for different query types. For descriptive queries (like \"setting\"), a finer granularity is usually better.\n2.  **Information Coherence:** Prioritize chunk sizes that promote the extraction of semantically coherent text segments. Each chunk should ideally contain information that is strongly related to a single concept or aspect of the narrative.\n3.  **Entity Relevance:** Aim for chunk sizes that ensure entities relevant to specific query types (e.g., locations, characters, themes) are not diluted by unrelated plot elements within the same chunk.\n4.  **Community Purity:** A smaller chunk size generally leads to more focused communities, enabling the LLM agent to select more relevant graph components. While this might increase the number of communities, it enhances the purity and distinctiveness of each.\n5.  **Balancing Act:** Avoid excessively small chunks that might break up essential contextual information. The goal is to find a balance that maximizes the signal-to-noise ratio for retrieved information.\n\n**Task:**\n\nBased on the current query and the above critique, **determine the most appropriate chunk size** for graph construction. Justify your choice by explaining how it addresses the identified issues and promotes better retrieval for the query \"Describe the setting of the story.\" Your reasoning should explicitly reference the concepts of granularity, information coherence, entity relevance, and community purity.\n```",
  "learned_prompt_answer_generator_graph": "You are an AI assistant specializing in generating rich, coherent, and atmospheric descriptions of story settings based on provided knowledge graph summaries.\n\n**Your Task:**\nGiven a user's query and a set of retrieved community summaries, synthesize this information into a single, flowing descriptive answer.\n\n**Input:**\n1.  **Query:** The user's question about the story's setting.\n2.  **Retrieved Information (Community Summaries):** Snippets of text describing various aspects of the story's world.\n\n**Instructions for Answer Generation:**\n\n1.  **Synthesize and Weave:** Combine related details from the retrieved information into a cohesive narrative. Avoid simply listing facts or enumerating elements.\n2.  **Focus on Narrative Relevance:** Prioritize descriptions that define the primary environments and significant locations central to the story. Integrate broader contextual elements (e.g., space, celestial bodies) only if they directly impact or are integral to the depicted narrative world.\n3.  **Evoke Atmosphere and Sensory Details:** Employ descriptive language to convey the mood, feeling, and key sensory experiences (visuals, sounds, smells, etc.) of the setting. Aim to immerse the reader.\n4.  **Establish Coherence and Structure:** Group similar locations or environmental features. Explain how different aspects of the setting relate to each other and contribute to the overall world-building.\n5.  **Prioritize Key Elements:** Focus on the most prominent, recurring, or impactful settings and details mentioned in the retrieved information.\n6.  **Direct Answer:** Provide only the generated descriptive answer. Do not include any introductory phrases (e.g., \"Based on the information provided...\") or concluding remarks.\n\n**Example Query:** \"Describe the main city where the protagonist lives.\"\n**Example Retrieved Information:**\n*   \"The city of Veridia is a sprawling metropolis characterized by towering chrome skyscrapers that pierce the perpetual smog. Its lower levels are a labyrinth of neon-lit alleyways and bustling marketplaces, filled with exotic aromas and the constant hum of unseen machinery.\"\n*   \"Veridia's sky-bridges connect the upper districts, offering breathtaking, albeit distant, views of the industrial wastelands that surround the city.\"\n*   \"The primary mode of transport within Veridia are magnetic levitation trains that glide silently between districts.\"\n\n**Desired Output (Illustrative):**\n\"Veridia is a vast, smog-choked metropolis, defined by its imposing chrome skyscrapers that scrape the sky. Its heart beats in the lower districtsâ€”a maze of vibrant, neon-drenched alleyways and active marketplaces, alive with exotic scents and the perpetual hum of unseen technology. Sleek magnetic levitation trains silently navigate between districts, while high above, sky-bridges link the affluent upper levels, offering glimpses of the desolate industrial expanse encircling this urban giant.\"",
  "learned_prompt_graph_retrieval_planner": "```\nYou are an expert community selection agent for a GraphRAG system. Your task is to select the most relevant communities from a graph to answer a user's query. You must be highly strategic, prioritize relevance, and avoid over-selection.\n\nWhen a user asks to \"Describe the setting of the story,\" your primary objective is to identify communities that provide **environmental, geographical, atmospheric, and integral world-building details** of the narrative's world.\n\n**Crucially, you must understand \"setting\" to encompass:**\n\n*   **Geographical Locations:** Specific planets, continents, cities, distinct natural landscapes (jungles, deserts, oceans, mountains), or unique environmental zones.\n*   **Atmosphere and Mood:** Descriptions that convey the prevailing feeling, tone, or sensory experience of a place (e.g., oppressive, serene, vibrant, desolate).\n*   **Integral World-Building:** Unique societal structures, cultural practices, or technological elements that fundamentally define the environment or how characters interact with it, *provided they describe the place itself*.\n\n**What to AVOID:**\n\n*   **Production/Meta Information:** Communities focused on how the story was made, its development, or authorial intent.\n*   **Purely Mechanical/Detail-Oriented:** Descriptions of specific, isolated mechanisms or objects that do not contribute to the broader environmental picture.\n*   **Plot-Centric Summaries:** Communities that primarily describe character actions, conflicts, political machinations, or plot events, even if they occur *in* a setting.\n*   **Character-Centric Summaries:** Communities focused on character motivations, relationships, or internal states unless directly tied to describing their environment.\n\n**Your Selection Strategy:**\n\n1.  **Prioritize Environmental Descriptions:** Communities that explicitly describe the *nature of the place* are of highest value.\n2.  **Be Highly Selective:** Aim for a *small, focused set* of communities (ideally 3-5) that *collectively* paint a comprehensive picture. Avoid a \"scattergun approach.\"\n3.  **Score Relevance Hierarchically:**\n    *   **High Relevance:** Direct descriptions of geography, landscape, climate, atmosphere, or defining world features.\n    *   **Medium Relevance:** Communities that describe how societies or cultures *live within* or *interact with* the environment in a way that shapes its character.\n    *   **Low Relevance:** Communities where the setting is merely a backdrop for plot or character action, or that focus on isolated details.\n4.  **Synthesize Information:** Consider how selected communities might combine to create a richer understanding. For example, if one community describes a jungle and another a specific type of flora/fauna found there, they work well together.\n5.  **Prune Ruthlessly:** Discard any community that does not directly contribute to describing the world's environment or atmosphere, even if it mentions a place name.\n\nFor the query \"Describe the setting of the story,\" meticulously evaluate each community. Select only those that offer direct, impactful insights into the *where* and the *feel* of the story's world, ensuring a coherent and representative overview.\n```",
  "learned_prompt_graph_builder": "You are an advanced Graph Construction Agent. Your task is to extract entities and their relationships from provided text to build a knowledge graph. The graph should be optimized for retrieving information about the *setting* and *atmosphere* of a story.\n\n**Extraction Guidelines:**\n\n1.  **Entity Types:** Extract the following entity types. For each entity, include a concise `description`.\n    *   `Person`: Individuals within the narrative.\n    *   `Organization`: Groups, factions, or institutions.\n    *   `Location`: Places where events occur.\n        *   **Location Properties:** For `Location` entities, prioritize and include *descriptive attributes* that define the setting. Essential properties include:\n            *   `name`: The explicit name of the location.\n            *   `type`: (e.g., `Natural`, `Artificial`, `Cosmic`, `Urban`, `Rural`, `Interior`, `Exterior`).\n            *   `climate`: (e.g., `temperate`, `arid`, `icy`, `tropical`, `toxic`).\n            *   `terrain`: (e.g., `mountainous`, `flat`, `forest`, `desert`, `underwater`, `urban landscape`).\n            *   `atmosphere`: (e.g., `oppressive`, `serene`, `chaotic`, `mysterious`, `technological`, `pristine`).\n            *   `dominant_features`: Key physical characteristics (e.g., `giant crystals`, `dense fog`, `advanced architecture`, `ancient ruins`).\n            *   `sensory_details`: Notable sights, sounds, smells, or tactile experiences associated with the location (e.g., `scent of ozone`, `constant hum`, `blinding sunlight`).\n            *   `era_period`: The time or historical context if discernible (e.g., `prehistoric`, `medieval`, `futuristic`, `Victorian`).\n    *   `Object`: Significant items, artifacts, or structures. Include properties like `material`, `function`, `age`, `condition`.\n    *   `Event`: Notable occurrences or actions. Include `time`, `location_ref`, `participants_ref`.\n    *   `Concept`: Abstract ideas, themes, or phenomena relevant to the setting (e.g., `magic system`, `societal norms`, `technological advancement`).\n    *   `EnvironmentalPhenomenon`: Specific atmospheric or geological conditions (e.g., `acid rain`, `sandstorm`, `zero-gravity`, `aurora`).\n\n2.  **Relationship Types:** Extract relationships between entities. For each relationship, include:\n    *   `source_entity_id`\n    *   `target_entity_id`\n    *   `type`: (e.g., `located_in`, `inhabited_by`, `associated_with`, `part_of`, `influences`, `experiences`, `created_by`, `depicts`).\n    *   `setting_relevance`: A numerical score (1-5) indicating how strongly this relationship defines or describes the setting or atmosphere. Higher scores indicate greater importance for setting description.\n    *   `description`: A brief natural language description of the relationship.\n\n**Prioritization:**\n\n*   Focus on extracting entities and relationships that directly contribute to understanding the *environment*, *atmosphere*, and *sensory experience* of the story's world.\n*   For `Location` entities, prioritize descriptive properties over functional ones.\n*   Favor relationships with high `setting_relevance`.\n\n**Output Format:**\n\nProvide the extracted information as a list of JSON objects, each representing either an entity or a relationship. Ensure all entities have a unique `id`. Relationships should reference these `id`s.\n\n**Example Entity (Location):**\n```json\n{\n  \"id\": \"loc_001\",\n  \"type\": \"Location\",\n  \"name\": \"Xylos Prime\",\n  \"description\": \"A desolate desert planet.\",\n  \"properties\": {\n    \"type\": \"Natural\",\n    \"climate\": \"arid\",\n    \"terrain\": \"desert\",\n    \"atmosphere\": \"oppressive\",\n    \"dominant_features\": [\"colossal rock spires\", \"twin suns\"],\n    \"sensory_details\": [\"scorching heat\", \"howling winds\", \"fine red dust\"],\n    \"era_period\": \"post-collapse\"\n  }\n}\n```\n\n**Example Relationship:**\n```json\n{\n  \"source_entity_id\": \"person_005\",\n  \"target_entity_id\": \"loc_001\",\n  \"type\": \"inhabited_by\",\n  \"setting_relevance\": 3,\n  \"description\": \"Kaelen's nomadic tribe inhabits the harsh deserts of Xylos Prime.\"\n}\n```",
  "learned_prompt_hyperparameters_vector": "",
  "learned_prompt_answer_generator_vector": "",
  "learned_prompt_vector_retrieval_planner": "",
  "hyperparameters_graph_agent_critique": "The chosen chunk size of 200 appears to be a contributing factor to the observed issues in the GraphRAG system, particularly concerning the relevance and focus of retrieved information for a query like \"Describe the setting of the story.\"\n\nHere's a critique of how the chunk size might be improved:\n\n1.  **Information Granularity and Context Loss:** A chunk size of 200 might be too large, potentially grouping together disparate pieces of information that are not strongly related. This can lead to communities that are a mix of setting details and plot elements, making it difficult for the LLM agent to discern what truly contributes to the \"setting.\" For instance, a single 200-token chunk might contain descriptions of a planet's geography alongside dialogue about a character's immediate actions. When these are merged into a graph and then clustered, the resulting community may become a muddled representation, lacking the focused descriptive power needed for a setting query.\n\n2.  **Entity and Relationship Density:** Larger chunks can result in a higher density of entities and relationships being extracted within a single chunk. While this might seem beneficial for capturing more information, it can also lead to \"noisy\" or less coherent subgraphs. If a chunk describes both the physical environment and a technical mechanism, the resulting graph edges might connect unrelated concepts. This can dilute the signal for specific entity types relevant to setting (e.g., Location, Environment) and introduce irrelevant ones (e.g., Object, Concept related to plot mechanics).\n\n3.  **Impact on Community Cohesion:** The community detection algorithm aims to group related entities. If the initial chunks are too broad, they may introduce entities and relationships that fragment the thematic coherence of potential communities. A chunk size that is too large might inadvertently \"bridge\" different conceptual areas, forcing entities that belong to distinct setting aspects (e.g., a desert landscape vs. a specific alien structure) into the same initial chunk, and subsequently, potentially the same community. This makes it harder for communities to represent a singular or cohesive aspect of the story's setting.\n\n4.  **Potential for Over-Summarization:** While summaries are an LLM task, the quality of the input to the summarization LLM is crucial. If communities are too broad due to large chunk sizes, the LLM tasked with summarizing them might struggle to create concise and relevant descriptions for a specific query type like \"setting.\" It might default to more general statements or include too many diverse elements, mirroring the incoherence of the underlying data.\n\n**Recommendation:**\n\nConsider experimenting with smaller chunk sizes (e.g., 100-150 tokens). This could lead to more granular extraction, potentially creating communities that are more semantically focused on specific aspects of the setting. It might also improve the LLM agent's ability to select truly relevant communities by presenting it with more distinct and less \"diluted\" options. A trade-off might be a slightly larger number of communities overall, but this could be managed by more effective agent selection.",
  "graph_builder_agent_critique": "The provided prompt for graph construction has several shortcomings that likely contribute to the identified issues in community selection and retrieval relevance, especially for a query like \"Describe the setting of the story.\"\n\nHere's a critique focused on improving the prompt for better graph construction:\n\n**Critique of the Graph Construction Prompt:**\n\n1.  **Lack of Granularity in Entity/Relationship Extraction for \"Setting\":** The prompt asks for general entity types (Person, Organization, Location, etc.) and relationships. However, for setting-related queries, it doesn't explicitly encourage the extraction of *descriptive properties* that define a setting. For example, a \"Location\" entity might have properties like \"climate,\" \"terrain,\" \"atmosphere,\" or \"dominant flora/fauna,\" which are crucial for setting description but aren't explicitly requested beyond a generic \"description\" field. Similarly, relationships could be more nuanced, describing *how* entities interact with the environment rather than just *that* they are related.\n\n2.  **Oversimplified Entity Typing:** While broad types are useful, a more detailed categorization could benefit setting analysis. For instance, distinguishing between \"Natural Location\" (e.g., planet, forest) and \"Artificial Location\" (e.g., city, spaceship interior) or categorizing environments (e.g., \"Atmospheric Condition,\" \"Terrain Type\") could allow the downstream agent to make more informed selections.\n\n3.  **No Emphasis on Contextual Importance:** The prompt asks for *all* relationships among *extracted* entities. This can lead to a graph dense with trivial or incidental relationships, especially when trying to focus on the setting. There's no instruction to prioritize relationships that *define the environment* or *contribute significantly to the atmosphere* of a scene. For instance, a relationship \"Person X visits Location Y\" is less informative for setting than \"Location Y has a harsh, icy climate.\"\n\n4.  **Implicit vs. Explicit Setting Elements:** The prompt relies on entities being explicitly typed as \"Location\" and their properties to convey setting. However, setting is also built through descriptions of objects, events, and even character perceptions that aren't directly \"Locations.\" For example, a \"strange object\" in a \"room\" might contribute to the setting's mood. The prompt doesn't guide the extraction of these more subtle setting elements or their properties.\n\n5.  **Limited Scope of \"Properties\":** The example properties (age, role, description) are generic. For setting, properties should be more specific and descriptive of the environment. The prompt should encourage capturing sensory details, scale, geological features, weather patterns, or the technological level of a location.\n\n**Recommendations for Prompt Improvement:**\n\n1.  **Enhanced Entity Properties for Setting:** Modify the prompt to explicitly ask for setting-specific properties for Location entities, such as: `climate`, `terrain`, `atmosphere`, `dominant features`, `sensory details`, `architectural style`, `era/time period`.\n2.  **Attribute Relationships to Setting Contribution:** For relationships, add a field like `setting_relevance` or `descriptive_value` that indicates how strongly the relationship contributes to defining the environment or atmosphere.\n3.  **Encourage \"Atmospheric\" Entities:** Prompt the extraction of entities that describe abstract environmental qualities, such as `WeatherPhenomenon`, `AtmosphericCondition`, `LightLevel`, `Soundscape`.\n4.  **Prioritize Descriptive Over Functional:** When extracting properties and relationships, implicitly or explicitly guide the model to favor descriptive details that build a sense of place over purely functional or plot-driven information.\n\nBy making these adjustments, the graph construction process can yield a knowledge graph that is richer in setting-specific details, allowing the community detection and retrieval agent to perform much more effectively for queries focused on the story's world.",
  "retrieval_planner_agent_critique": "The community selection for the query \"Describe the setting of the story\" demonstrates a critical misunderstanding of the query's intent and the nature of \"setting\" in a narrative context. The selection is excessively broad, lacks strategic focus, and includes communities that are irrelevant to describing a story's setting.\n\n**Key Issues:**\n\n1.  **Over-Selection and Lack of Prioritization:** Selecting ten communities (0-5, 7-10, 11-12, 16) for a single query is a significant overreach. This approach, described as a \"scattergun approach,\" inundates the system with potentially disconnected information, hindering the generation of a coherent setting description. The agent failed to prioritize communities offering core environmental or geographical context.\n\n2.  **Inclusion of Irrelevant Communities:** Several selected communities do not contribute to describing a story's setting:\n    *   **Community 0 (Raiders of the Second Moon Production):** This focuses on the *creation and production* of a work, not its narrative setting.\n    *   **Community 16 (Prison Door Security Mechanism):** This is a purely *mechanical description* of a detail within a potential setting, offering no atmospheric or environmental insight.\n    *   **Community 11 (Doctor Von Mark's African Nazi Conspiracy):** This emphasizes *plot and political conspiracy* over environmental description.\n    *   **Community 12 (Priests, Guards, and Sacred Artifacts in Conflict):** While potentially *occurring within* a setting, the summary itself focuses on character interactions and plot elements (artifacts, conflict) rather than the setting's characteristics.\n\n3.  **Poor Definition of \"Setting\":** The agent appears to interpret \"setting\" as any mention of a place or entity, irrespective of whether the information contributes to a description of the environment, atmosphere, or world-building. This broad interpretation leads to the inclusion of communities that are more about character actions (Community 1), plot events, or production details.\n\n4.  **Missed Opportunity for Synthesis:** A good setting description often involves synthesizing information about different aspects of the world (e.g., geographical regions, dominant cultures, specific landmarks, atmospheric qualities). By selecting a large, unfocused list, the agent missed the opportunity to identify and group communities that *collectively* paint a richer, more cohesive picture of the story's world. For instance, multiple communities mention islands, temples, or journeys, which could have been synthesized.\n\n**Recommendations for Improvement:**\n\n*   **Refine \"Setting\" Definition for the Agent:** The prompt needs to guide the agent to understand \"setting\" as encompassing:\n    *   **Geographical Locations:** Planets, continents, cities, natural landscapes, specific environments (e.g., jungle, desert, marsh).\n    *   **Atmosphere and Mood:** The general feeling or tone of a place.\n    *   **World-Building Details:** Societal structures, cultural elements, or unique characteristics of the world that are integral to the environment.\n*   **Implement Strategic Pruning:** The agent must be instructed to be highly selective, aiming for a *smaller, focused set* (ideally 3-5 communities) that *collectively* provide a comprehensive overview. It should prioritize communities that offer *environmental descriptions* over those focused on plot, character actions, or production details.\n*   **Introduce Hierarchical Relevance Scoring:** The agent should differentiate between communities that *describe the environment* versus those that *occur within an environment*. For a \"setting\" query, the former should be heavily weighted.\n*   **Encourage Synthesis:** Prompt the agent to think about how selected communities *combine* to describe the world. For example, if multiple communities describe different types of natural landscapes, they should be considered together.\n\nThe current selection is too broad and lacks the discernment needed for effective retrieval for the specific query of \"setting.\"",
  "answer_generation_critique": "The current prompt for the answer generation LLM is too generic. It instructs the LLM to use retrieved context to answer a query but lacks specific guidance on *how* to synthesize and present that information, especially for qualitative aspects like describing a setting.\n\nHere's a critique and suggestions for prompt improvement:\n\n**Critique of Current Prompt:**\n\n1.  **Lack of Synthesis Instruction:** The prompt tells the LLM to \"exploit the retrieved information\" but doesn't emphasize synthesizing it. The feedback highlights that the answer was an enumeration, not a cohesive narrative. The prompt needs to explicitly ask for synthesis.\n2.  **No Guidance on Tone or Atmosphere:** The feedback points out the absence of depth, coherence, and atmosphere. The current prompt doesn't mention anything about conveying mood, sensory details, or the overall feeling of the setting.\n3.  **Insufficient Emphasis on \"Story Setting\":** The feedback notes that \"space\" and \"other celestial bodies\" might be too broad if not central to the *story's* immediate environment. The prompt could be improved by directing the LLM to focus on the primary narrative environments.\n4.  **No Instruction on Prioritization or Grouping:** The feedback suggests grouping related locations and prioritizing key settings. The prompt doesn't guide the LLM to do this, leading to a long, disjointed list.\n5.  **Over-reliance on Direct Extraction:** The prompt implicitly allows for direct extraction by asking the LLM to \"exploit the retrieved information.\" It doesn't encourage rephrasing, contextualizing, or creative description.\n\n**Suggestions for Improved Prompt:**\n\nThe prompt needs to be more directive, guiding the LLM towards a more qualitative and synthesized output.\n\n**Revised Prompt Example:**\n\n\"You will be given a query and a set of summarized community descriptions from a knowledge graph. Your goal is to provide a **rich, coherent, and atmospheric description** that directly answers the query by **synthesizing** the provided information.\n\n**Query:**\n{}\n\n**Retrieved Information (Community Summaries):**\n{}\n\n**Instructions for Answer Generation:**\n\n1.  **Synthesize and Weave:** Do not simply list extracted details. Combine related information to create a flowing narrative description.\n2.  **Focus on the Story's Setting:** Prioritize descriptions that establish the primary environments where the story takes place. If broader contexts (like space) are mentioned, integrate them seamlessly only if they are integral to the depicted narrative world.\n3.  **Evoke Atmosphere and Sensory Details:** Use descriptive language to convey the mood, feeling, and key sensory aspects (visuals, sounds, etc.) of the setting. Imagine you are describing it to someone who hasn't read the story.\n4.  **Establish Coherence:** Group similar locations or environments. Explain how different parts of the setting relate to each other within the story's context.\n5.  **Prioritize Key Elements:** Focus on the most significant or recurring aspects of the setting mentioned in the retrieved information.\n6.  **Respond solely with the generated answer.** Do not include any introductory phrases like 'Based on the information provided...' or concluding remarks.\"\n\nThis revised prompt explicitly addresses the identified weaknesses by demanding synthesis, atmospheric language, focus on narrative relevance, and coherent structure, which should lead to more descriptive and less enumerative answers.",
  "graph_builder_prompt": "\nYou will be given a text. Your goal is to identify entities in the text and all the relationships among the identified entities.\nFor each entity, you will include:\n- name: the entity name\n- type: the entity type (e.g., Person, Organization, Location, Event, Concept)\n- properties: a list of key-value pairs describing characteristics of the entity extracted from the text (e.g., for a person: age, role, description; for a location: description, significance). Each property should have a \"key\" and \"value\" field.\n\nFor each relationship, you will include its type, a description (why you think the two entities are related to each other), and the evidence from the text that supports this.\nThe relationships must be among the extracted entities.\nProvide a list of triplets in your answer.\n\nReturn no more than 20 entities and 30 relationships. \n\nText:\n{TEXT_CHUNK}\n\nProvide the reasoning that led to your response.\n",
  "retrieval_prompt": "\nYou are an agentic retrieval component of a community-based GraphRAG system. Your goal is to select the most relevant communities from the knowledge graph to answer the following query: Describe the setting of the story.\nYou can select more than one community. \n\nAvailable Communities:\n{RETRIEVED_CONTEXT}\n\nSelect the communities that are most relevant to answering the query. Choose communities that together provide comprehensive coverage of the information needed.\n\nProvide the list of community IDs you want to retrieve and explain your reasoning for selecting these specific communities.\n"
}