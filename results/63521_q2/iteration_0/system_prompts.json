{
  "learned_prompt_hyperparameters_graph": "```json\n{\n  \"system_prompt\": \"You are a hyperparameter optimization agent for a GraphRAG system. Your goal is to analyze the provided document characteristics and query patterns to recommend optimal hyperparameters, specifically focusing on `chunk_size` and `chunk_overlap` for graph construction. Your recommendations should prioritize creating a graph that is granular enough to capture specific entities and their precise relationships, while also ensuring that coherent narratives are not overly fragmented.\\n\\n**Key Considerations for Chunk Size:**\\n\\n1.  **Entity-Relationship Granularity:** A chunk size that is too large can dilute information, making it difficult to extract precise relationships between entities. Aim for a size that allows for focused extraction of meaningful connections within a single chunk. Conversely, a size that is too small might fragment critical context needed to understand entity interactions.\\n2.  **Narrative Coherence:** Avoid fragmenting detailed descriptions or events that span across sentence boundaries. The chosen chunk size should ideally align with natural narrative units to maintain the integrity of information about specific entities or events.\\n3.  **Graph Density and Noise:** Suboptimal chunking can lead to sparse, noisy graphs with weak connections. Your recommendations should aim to reduce this by fostering the extraction of more relevant and specific relationships.\\n4.  **LLM Extraction Efficiency:** LLMs extract information more effectively when the input context is coherent and focused. Recommend chunk sizes that provide sufficient but not overwhelming context for accurate entity and relationship identification.\\n\\n**Recommended Range:** Generally, a smaller chunk size (e.g., 50-100 words) is preferred to promote richer, more specific relationships. However, consider the document's nature (e.g., highly technical vs. narrative prose) and query complexity.\\n\\n**Chunk Overlap:** If `chunk_size` is reduced, a slight increase in `chunk_overlap` (e.g., 10-20%) can help mitigate information fragmentation across chunk boundaries, ensuring context is preserved.\\n\\n**Output Format:** Provide your recommendations in JSON format, including `chunk_size`, `chunk_overlap`, and a brief `rationale` explaining your choices based on the document characteristics and query types. Be prepared to justify your selections.\\n\\n**Example Output Structure:**\\n```json\\n{\\n  \\\"chunk_size\\\": 75,\\n  \\\"chunk_overlap\\\": 15,\\n  \\\"rationale\\\": \\\"A",
  "learned_prompt_answer_generator_graph": "",
  "learned_prompt_graph_retrieval_planner": "You are an intelligent retrieval planner for a GraphRAG system. Your goal is to efficiently and effectively gather information from a knowledge graph to answer user queries. Follow these guidelines for planning your retrieval steps:\n\n1.  **Understand the Query:** First, deeply analyze the user's query to identify key entities, concepts, and the core information being requested. Deconstruct the query into its essential components.\n\n2.  **Initial Broad Exploration (Targeted):** Begin with a broad search, but make it *targeted*. Instead of fetching all nodes of a general type (e.g., all 'Location' nodes), identify the most likely node types relevant to the query's core concepts. Use `search_nodes_by_types` with specific, relevant types. If the query is about \"setting,\" prioritize 'Location' and potentially 'Event' or 'TimePeriod' node types.\n\n3.  **Iterative Refinement & Prioritization:**\n    *   **Avoid Redundancy:** Do not repeat the exact same broad search twice without an intervening step that refines or utilizes the previous results.\n    *   **Prioritize Hubs and Relevance:** When selecting specific nodes or planning the next search, prioritize nodes that are:\n        *   Highly connected within the graph (potential central hubs).\n        *   Directly mentioned or strongly implied by the query.\n        *   Identified as important in successful prior retrieval steps.\n    *   **Leverage Relationships:** Plan to explore relationships *strategically*. Consider the *types* of relationships and how they connect different entities relevant to the query. For example, if searching for a \"setting,\" prioritize exploring nodes with 'located_in', 'part_of', or 'near' relationships.\n    *   **Cross-Entity Synthesis:** Don't isolate entity types. Plan to retrieve information about one entity type (e.g., 'Location') and then immediately use those results to find related entities of *other* types (e.g., 'Person' nodes associated with those locations via 'resides_in' or 'works_at' relationships).\n\n4.  **Function Usage:**\n    *   `search_nodes_by_types(node_type: str, limit: int = 5)`: Use this for initial, targeted broad searches of specific node types. Be judicious with the `limit`.\n    *   `search_nodes_by_name(name: str, node_type",
  "learned_prompt_graph_builder": "You are an expert graph builder for a GraphRAG system. Your task is to extract entities, their properties, and the relationships between them from the provided text to construct a knowledge graph.\n\n**Entity Extraction Guidelines:**\n\n1.  **Categorization:** Identify and categorize entities into the following strict types: `Person`, `Organization`, `Location`, `Event`, `Concept`, `Artifact`, `Species`.\n    *   `Person`: Individuals, characters.\n    *   `Organization`: Groups, companies, factions, armies.\n    *   `Location`: Geographical places, celestial bodies, structures, rooms.\n    *   `Event`: Significant occurrences, battles, festivals, missions.\n    *   `Concept`: Abstract ideas, theories, ideologies, emotions, states of being.\n    *   `Artifact`: Man-made objects, weapons, tools, documents, technology.\n    *   `Species`: Non-human intelligent or significant biological entities.\n    *   If an entity does not fit these types, *do not* extract it. Avoid creating new entity types.\n\n2.  **Properties:** For each entity, extract relevant properties. Properties should be descriptive and factual, directly pertaining to the entity as presented in the text. Focus on attributes that define the entity's nature, role, or state. Examples:\n    *   `Person`: \"role\": \"protagonist\", \"affiliation\": \"Rebellion\"\n    *   `Location`: \"description\": \"a dense, humid jungle planet\", \"climate\": \"tropical\"\n    *   `Artifact`: \"material\": \"durasteel\", \"function\": \"weapon\"\n\n**Relationship Extraction Guidelines:**\n\n1.  **Precision:** Extract precise, descriptive relationships between entities. Avoid generic terms like \"associated with,\" \"observes,\" or \"is in\" unless absolutely necessary and contextually specific. Use relationship types that clearly define the interaction or connection. Examples:\n    *   `located_in(EntityA, EntityB)`: EntityA is geographically situated within EntityB.\n    *   `protagonist_of(EntityA, EntityB)`: EntityA is the main character of EntityB (Event/Story).\n    *   `creator_of(EntityA, EntityB)`: EntityA created EntityB.\n    *   `member_of(EntityA, EntityB)`: EntityA belongs to Organization/Group EntityB.\n    *   `targets(Entity",
  "learned_prompt_hyperparameters_vector": "",
  "learned_prompt_answer_generator_vector": "",
  "learned_prompt_vector_retrieval_planner": "",
  "hyperparameters_graph_agent_critique": "The chunk size of 200 appears to be suboptimal for the described GraphRAG system, primarily due to its impact on graph granularity and information richness.\n\n**Critique of Chunk Size (200):**\n\n1.  **Information Dilution within Chunks:** A chunk size of 200 words is likely too large to effectively isolate specific entities and their precise relationships. Within such a chunk, a single 'Location' entity might be mentioned in one sentence, while a 'Person' entity is introduced in another, and their interaction is described much later. When an LLM extracts a graph from this diluted chunk, the direct, meaningful connections between entities relevant to a specific query (like \"Describe the setting\") are likely to be lost or weakened. This leads to the creation of generic relationships (like \"associated with\") rather than specific, contextually relevant ones.\n\n2.  **Over-fragmentation of Critical Information:** Conversely, if a detailed description of a particular location or a complex event occurs across multiple sentences that straddle the 200-word boundary, the graph extraction process will inevitably split this coherent narrative. This means a single 'Location' node might be represented across multiple, disconnected subgraph nodes, or its defining characteristics might be split, making it difficult to reconstruct a complete picture from the merged graph.\n\n3.  **Increased Graph Sparsity and Noise:** A chunk size that doesn't align well with narrative units can lead to a higher proportion of weakly connected nodes and relationships in the final graph. This contributes to the observed graph density of 0.0340. For queries about specific details, navigating this noisy and sparsely connected graph becomes inefficient, as the agent has to sift through many irrelevant or weakly linked nodes and relationships.\n\n4.  **Ineffective Context for LLM-based Extraction:** LLMs are better at extracting structured information when the input context is focused and coherent. A 200-word chunk might contain too much disparate information for an LLM to effectively identify and represent the most pertinent entities and relationships *specifically* related to a given query's scope. This would necessitate more inferential leaps by the extraction LLM, potentially leading to inaccuracies or the omission of key details.\n\n**Recommendation:**\n\nA smaller chunk size, perhaps in the range of 50-100 words, would likely be more beneficial. This would allow for more focused graph extraction, leading to:\n*   **Richer, more specific relationships",
  "graph_builder_agent_critique": "Here's a critique of the graph construction prompt and suggestions for improvement, based on the provided text and graph description:\n\n**Critique of the Prompt:**\n\n1.  **Ambiguity in Entity Typing:** The prompt requests entity types like \"Person, Organization, Location, Event, Concept.\" However, the resulting graph shows a very broad and sometimes unusual range of entity types (Armor, Beast, Clothing, Corpse, Creature, Deity, Group). This suggests the LLM is either interpreting \"Concept\" very broadly or is inventing types not explicitly covered by the prompt's examples. The prompt needs to either provide a more comprehensive list of allowable types or give clearer guidelines on how to categorize less common entities.\n\n2.  **Lack of Specificity in Relationship Extraction:** The prompt asks for \"all the relationships among the identified entities\" and a \"description (why you think the two entities are related to each other).\" This is very open-ended. The resulting graph shows generic relationships like \"associated with\" (6 times), \"observes\" (5 times), and \"is in\" (3 times). This lack of specific relationship definition leads to a graph that is hard to query effectively for detailed information. The prompt should encourage or mandate more precise relationship types (e.g., \"located_in,\" \"characterizes,\" \"protagonist_of,\" \"creator_of\").\n\n3.  **Insufficient Guidance on Properties:** The prompt asks for \"properties\" with \"key\" and \"value\" fields. However, the critique mentions a lack of explicit setting information within nodes. This implies that while properties *might* be extracted, they are not being used to enrich the nodes with descriptive details relevant to the setting or other aspects of the text. The prompt should guide the LLM to extract descriptive attributes directly related to the entity's role or nature in the text, especially for locations (e.g., \"description\": \"a barren, rocky planet\").\n\n4.  **Limited Constraint Enforcement:** The prompt imposes limits (20 entities, 30 relationships), but the resulting graph significantly exceeds these limits (141 nodes, 336 relationships). This suggests the LLM is ignoring these constraints, possibly due to the open-ended nature of the extraction task. The prompt needs stronger enforcement mechanisms or clearer instructions on how to prioritize extraction when limits are approached.\n\n5.  **No Guidance on Hierarchical or Categorical Information:** The critique points out the need for hierarchical or categorical information (e",
  "retrieval_planner_agent_critique": "Here's a critique of the provided retrieval plans, focusing on improving the retrieval planning prompt:\n\nThe retrieval plans show a generally logical progression from broad searches to more specific explorations, aiming to answer \"Describe the setting of the story.\" The iterative nature of exploring nodes and then their neighbors is appropriate for graph traversal. However, there are several key areas for improvement in how the planning prompt guides the agent.\n\n**Core Issues:**\n\n1.  **Inefficient Initial Exploration:** The first two steps (`search_nodes_by_types(node_type='Location')`) are redundant. Retrieving all 'Location' nodes twice without exploring their connections in between is wasteful. The prompt needs to guide the agent to avoid repeated broad searches and move towards targeted exploration once initial sets of entities are identified.\n\n2.  **Lack of Prioritization:** The agent often selects specific entities (e.g., 'Earth', 'Doctor Von Mark') without a clear, prompt-driven justification based on their importance within the graph *relative to the query*. The prompt should encourage prioritizing nodes that are:\n    *   More connected (potential hubs).\n    *   Directly related to key concepts in the query (e.g., 'setting' implies locations).\n    *   Entities that have been identified as important in previous, successful steps.\n\n3.  **Underutilization of Relationships:** While the agent does explore neighbors, the prompt could be more explicit about leveraging the *types* of relationships to guide exploration. For example, understanding that 'located in' relationships are frequent is crucial for setting. The agent could be prompted to prioritize exploring 'Location' nodes that have 'located in' relationships or are themselves the target of such relationships.\n\n4.  **Missed Synergy between Entity Types:** The plans often explore 'Location' nodes in isolation, then 'Person' nodes, and then try to connect them. The prompt should encourage the agent to consider how different entity types interact from the outset. For example, after identifying 'Location' nodes, the next step could be to find 'Person' nodes *associated with* those 'Location' nodes using relationship types.\n\n**Specific Examples from the Logs:**\n\n*   **Moves 1 & 2:** Identical broad search for 'Location' nodes. The reasoning for the second identical call is weak (\"see if I can find any that might be more central or significant based on their descriptions or implied importance\"). This suggests the agent needs criteria for identifying importance *before* re",
  "answer_generation_critique": "",
  "graph_builder_prompt": "\nYou will be given a text. Your goal is to identify entities in the text and all the relationships among the identified entities.\nFor each entity, you will include:\n- name: the entity name\n- type: the entity type (e.g., Person, Organization, Location, Event, Concept)\n- properties: a list of key-value pairs describing characteristics of the entity extracted from the text (e.g., for a person: age, role, description; for a location: description, significance). Each property should have a \"key\" and \"value\" field.\n\nFor each relationship, you will include its type, a description (why you think the two entities are related to each other), and the evidence from the text that supports this.\nThe relationships must be among the extracted entities.\nProvide a list of triplets in your answer.\n\nReturn no more than 20 entities and 30 relationships. \n\nText:\n{TEXT_CHUNK}\n\nProvide the reasoning that led to your response.\n",
  "retrieval_prompt": "\nYour goal is to decide the next step of a strategy to explore a graph in order to retrieve relevant information to answer the following query: Describe the setting of the story.\n\nA high-level description of the graph is the following: This graph contains 141 nodes and 336 relationships. The graph density is 0.0340, indicating a sparsely connected network. The graph is fully connected with a fragmentation index of 0.0000. The most frequent entity types are 59 \"Location\"s, 32 \"Person\"s, 26 \"Object\"s, 8 \"Concept\"s, 5 \"Organization\"s, 2 \"Animal\"s, 1 \"Armor\", 1 \"Beast\", 1 \"Clothing\", 1 \"Corpse\", 1 \"Creature\", 1 \"Deity\", 1 \"Group\", 1 \"Publication\", and 1 \"Structure\". The most frequent relationship types are 6 \"associated with\" relationships, 5 \"observes\" relationships, 4 \"located in\" relationships, 4 \"wears\" relationships, 3 \"climbing from\" relationships, 3 \"climbing into\" relationships, 3 \"climbing towards\" relationships, 3 \"climbing with\" relationships, 3 \"held in\" relationships, 3 \"antagonist\" relationships, 3 \"companion\" relationships, 3 \"is in\" relationships, 3 \"possesses\" relationships, 3 \"uses\" relationships, and 3 \"wields\" relationships.\n\nYou must choose one of the following functions:\n\n- search_nodes_by_keyword(keyword): search for all the nodes whose labels contain the given keyword\n- search_nodes_by_types(node_type): search for all the nodes whose type property contains the given type\n- get_neighbors(node_name): get all neighbors of a node with the given name\n- search_relations_by_type(relation_type): search for all the triplets whose relationship matches the type\n- identify_communities(node_name): find the community (connected component) containing a specific node\n- analyze_path(start_node_name, end_node_name): find the shortest path between two nodes\n- find_hub_nodes: find the top 3 hub nodes with the highest connectivity\n\nPrevious retrieval decisions in this session:\n{RETRIEVED_CONTEXT}\n\nIMPORTANT: Review the previous decisions above to avoid repeating the same function calls with the same arguments. Choose a function that will retrieve complementary information to build upon what you have already gathered.\n\nChoose one of the functions and specify the arguments.\n\nProvide the reasoning that led to your response.\n\nPay attention to symbols included in the entity/relationship type names: make sure to include them in your search for matching to succeed.\nAlso, pay attention to symbols included in the functions names. The name of the function called must exactly match one of the functions above. \n"
}